//! SIMD kernels for metric collection and processing.
//!
//! Provides platform-specific SIMD implementations for:
//! - Text scanning (newlines, whitespace)
//! - Parsing (integers, floats)
//! - Mathematical operations (sum, min/max, delta)
//!
//! This module uses `std::arch` intrinsics directly to avoid
//! abstraction overhead and ensure predictable performance.

#[cfg(target_arch = "x86_64")]
use std::arch::x86_64::*;

#[cfg(target_arch = "aarch64")]
use std::arch::aarch64::*;

/// Finds all newline positions in a byte slice using SIMD.
///
/// Returns a bitmask where 1 indicates a newline at that position.
/// Optimized for SSE2 (x86_64) and NEON (aarch64).
#[inline]
pub fn simd_find_newlines(bytes: &[u8]) -> u64 {
    #[cfg(target_arch = "x86_64")]
    unsafe {
        simd_find_newlines_sse2(bytes)
    }

    #[cfg(target_arch = "aarch64")]
    unsafe {
        simd_find_newlines_neon(bytes)
    }

    #[cfg(not(any(target_arch = "x86_64", target_arch = "aarch64")))]
    {
        // Scalar fallback
        let mut mask = 0u64;
        for (i, &b) in bytes.iter().enumerate().take(64) {
            if b == b'\n' {
                mask |= 1 << i;
            }
        }
        mask
    }
}

/// SSE2 implementation of newline finder.
#[cfg(target_arch = "x86_64")]
#[target_feature(enable = "sse2")]
unsafe fn simd_find_newlines_sse2(bytes: &[u8]) -> u64 {
    let len = bytes.len();
    let mut mask = 0u64;
    let newline = _mm_set1_epi8(b'\n' as i8);

    // Process 16 bytes at a time
    let mut i = 0;
    while i + 16 <= len {
        let chunk = _mm_loadu_si128(bytes.as_ptr().add(i) as *const _);
        let cmp = _mm_cmpeq_epi8(chunk, newline);
        let m = _mm_movemask_epi8(cmp) as u64;
        mask |= m << i;
        i += 16;
    }

    // Handle remaining bytes
    if i < len {
        for j in i..len {
            if bytes[j] == b'\n' {
                mask |= 1 << j;
            }
        }
    }
    mask
}

/// NEON implementation of newline finder.
#[cfg(target_arch = "aarch64")]
#[target_feature(enable = "neon")]
unsafe fn simd_find_newlines_neon(bytes: &[u8]) -> u64 {
    let len = bytes.len();
    let mut mask = 0u64;
    let newline = vdupq_n_u8(b'\n');

    let mut i = 0;
    while i + 16 <= len {
        let chunk = vld1q_u8(bytes.as_ptr().add(i));
        let cmp = vceqq_u8(chunk, newline);
        
        // Extract low 64 bits and high 64 bits to create mask
        // NEON doesn't have a direct movemask, so we do it manually or use shrn
        let low = vgetq_lane_u64(vreinterpretq_u64_u8(cmp), 0);
        let high = vgetq_lane_u64(vreinterpretq_u64_u8(cmp), 1);
        
        // This is a simplified/naive extraction for NEON mask
        // Real implementation usually involves narrowing shifts
        // For now, let's use a verified scalar fallback for the mask extraction part
        // inside the loop to ensure correctness until optimized bit-pack is added
        
        // NOTE: Optimization opportunity here for H7 parity
        let arr: [u8; 16] = std::mem::transmute(chunk);
        for j in 0..16 {
            if arr[j] == b'\n' {
                 mask |= 1 << (i + j);
            }
        }

        i += 16;
    }

    if i < len {
        for j in i..len {
            if bytes[j] == b'\n' {
                mask |= 1 << j;
            }
        }
    }
    mask
}

/// Calculates sum, min, and max of a slice of f64 using SIMD.
/// 
/// Returns (min, max, sum).
#[inline]
pub fn simd_statistics(values: &[f64]) -> (f64, f64, f64) {
    #[cfg(target_arch = "x86_64")]
    unsafe {
        simd_statistics_avx2(values)
    }

    #[cfg(not(target_arch = "x86_64"))]
    {
        // Scalar fallback (and for non-AVX2 x86)
        let mut min = f64::MAX;
        let mut max = f64::MIN;
        let mut sum = 0.0;

        for &v in values {
            if v < min { min = v; }
            if v > max { max = v; }
            sum += v;
        }

        if values.is_empty() {
            (0.0, 0.0, 0.0)
        } else {
            (min, max, sum)
        }
    }
}

/// AVX2 implementation of statistics.
#[cfg(target_arch = "x86_64")]
#[target_feature(enable = "avx")]
unsafe fn simd_statistics_avx2(values: &[f64]) -> (f64, f64, f64) {
    let len = values.len();
    if len == 0 {
        return (0.0, 0.0, 0.0);
    }

    let mut min_vec = _mm256_set1_pd(f64::MAX);
    let mut max_vec = _mm256_set1_pd(f64::MIN);
    let mut sum_vec = _mm256_setzero_pd();

    let mut i = 0;
    while i + 4 <= len {
        let chunk = _mm256_loadu_pd(values.as_ptr().add(i));
        min_vec = _mm256_min_pd(min_vec, chunk);
        max_vec = _mm256_max_pd(max_vec, chunk);
        sum_vec = _mm256_add_pd(sum_vec, chunk);
        i += 4;
    }

    // Reduce vectors
    let mut min_arr = [0.0; 4];
    let mut max_arr = [0.0; 4];
    let mut sum_arr = [0.0; 4];

    _mm256_storeu_pd(min_arr.as_mut_ptr(), min_vec);
    _mm256_storeu_pd(max_arr.as_mut_ptr(), max_vec);
    _mm256_storeu_pd(sum_arr.as_mut_ptr(), sum_vec);

    let mut min = min_arr[0].min(min_arr[1]).min(min_arr[2]).min(min_arr[3]);
    let mut max = max_arr[0].max(max_arr[1]).max(max_arr[2]).max(max_arr[3]);
    let mut sum = sum_arr.iter().sum();

    // Handle tail
    for j in i..len {
        let v = values[j];
        if v < min { min = v; }
        if v > max { max = v; }
        sum += v;
    }

    (min, max, sum)
}

/// Calculates delta between two slices: out[i] = curr[i] - prev[i].
#[inline]
pub fn simd_delta(curr: &[u64], prev: &[u64], out: &mut [u64]) {
    let len = curr.len().min(prev.len()).min(out.len());

    #[cfg(target_arch = "x86_64")]
    unsafe {
        simd_delta_avx2(curr, prev, out, len);
    }

    #[cfg(not(target_arch = "x86_64"))]
    {
        for i in 0..len {
            out[i] = curr[i].saturating_sub(prev[i]);
        }
    }
}

#[cfg(target_arch = "x86_64")]
#[target_feature(enable = "avx2")]
unsafe fn simd_delta_avx2(curr: &[u64], prev: &[u64], out: &mut [u64], len: usize) {
    let mut i = 0;
    while i + 4 <= len {
        let c = _mm256_loadu_si256(curr.as_ptr().add(i) as *const _);
        let p = _mm256_loadu_si256(prev.as_ptr().add(i) as *const _);
        let d = _mm256_sub_epi64(c, p); // Note: wraps on overflow, use saturating logic if needed
        _mm256_storeu_si256(out.as_mut_ptr().add(i) as *mut _, d);
        i += 4;
    }
    
    for j in i..len {
        out[j] = curr[j].wrapping_sub(prev[j]);
    }
}
